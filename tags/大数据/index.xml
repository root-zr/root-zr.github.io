<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>大数据 on Albert Wang</title>
    <link>https://root-zr.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/</link>
    <description>Recent content in 大数据 on Albert Wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Jul 2022 13:39:37 +0800</lastBuildDate><atom:link href="https://root-zr.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Slide for Zookeeper Learning | zookeeper学习中的PPT分享</title>
      <link>https://root-zr.github.io/en/2022/07/07/a-slide-for-zookeeper-learning-zookeeper%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84ppt%E5%88%86%E4%BA%AB/</link>
      <pubDate>Thu, 07 Jul 2022 13:39:37 +0800</pubDate>
      
      <guid>https://root-zr.github.io/en/2022/07/07/a-slide-for-zookeeper-learning-zookeeper%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84ppt%E5%88%86%E4%BA%AB/</guid>
      <description>这是一篇用来测试共享文件的博客，分享了一篇关于 zookeeper 分布式组件学习的PPT。
在PPT制作过程中参考了尚硅谷的教学材料【尚硅谷】大数据技术之Zookeeper 3.5.7版本教程_哔哩哔哩_bilibili 和 Raft 以及 zookeeper 的论文原文。
PPT 采用zoho 来托管，您可以点击这里 获取到PPT原文
 您也可以通过鼠标右键下面的内容，另存为pdf版本的文件。</description>
    </item>
    
    <item>
      <title>Distributed System and MapReduce 02 | 分布式系统和MapReduce 02</title>
      <link>https://root-zr.github.io/en/2022/06/15/distributed-system-and-mapreduce-02-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%92%8Cmapreduce-02/</link>
      <pubDate>Wed, 15 Jun 2022 13:39:37 +0800</pubDate>
      
      <guid>https://root-zr.github.io/en/2022/06/15/distributed-system-and-mapreduce-02-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%92%8Cmapreduce-02/</guid>
      <description>这篇博客主要是关于 MIT 的公开课 6.824 分布式系统 的 lab 1相关内容，lab 1 要求我们实现一个分布式的mapreduce系统，并且已经为我们提供了一些基础的代码，lab 的地址请点击这里 。下面让我们从文件的架构开始一点点去了解我们所要做的事情。下面是一个精简了的代码结构图，它可以帮助我们理解 lab 1的内容。
单机mapreduce任务 首先我们需要明确的是main目录下的文件是我们的程序入口处， 如果我们执行mrsequential.go 文件将会启动一个单机的mapreduce任务，执行的命令如下
go run -race mrsequential.go wc.so pg*.txt 可以看到在执行的时候还传了两个参数，这两个参数的作用是什么呢？我们从代码中来观察一下，
if len(os.Args) &amp;lt; 3 { fmt.Fprintf(os.Stderr, &amp;quot;Usage: mrsequential xxx.so inputfiles...\n&amp;quot;) os.Exit(1) } mapf, reducef := loadPlugin(os.Args[1]) 可以看到 wc.so 是用来加载 map 和 reduce 函数的， 而 wc.so 文件则是由下面这个命令生成的，通过-buildmode=plugin可以以插件的形式将wc.go 文件里的函数封装起来，然后通过loadPlugin 函数重新加载出来。
 go build -race -buildmode=plugin ../mrapps/wc.go loadPlugin 函数是作者自己编写的，具体是封装了plugin.Open 和 p.Lookup两个函数的功能，最终的结果是获取到map 和 reduce 两个函数。
func loadPlugin(filename string) (func(string, string) []mr.</description>
    </item>
    
    <item>
      <title>Distributed System and MapReduce 01 | 分布式系统和MapReduce 01</title>
      <link>https://root-zr.github.io/en/2022/04/20/distributed-system-and-mapreduce-01-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%92%8Cmapreduce-01/</link>
      <pubDate>Wed, 20 Apr 2022 13:39:37 +0800</pubDate>
      
      <guid>https://root-zr.github.io/en/2022/04/20/distributed-system-and-mapreduce-01-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%92%8Cmapreduce-01/</guid>
      <description>这篇博客主要是学习MIT的公开课6.824分布式系统 的学习笔记，在课程一开始老师主要介绍了分布式系统的和MapReduce的基本概念，后续有关于实现一个具体的MapReduce的实验。
分布式系统的基本概念 首先当用户要访问信息的时候，一般需要一个WEB的网页接口去和后台的数据库操作。当用户量很大的时候一台机器就不足以支撑这么大的并发量了，同时数据库所在的服务器也有可能会出现问题。所以很自然的想法就是用多台机器来分担压力。如果足够理想，两台机器就可以把压力减小一般，一直这样下去……不过可惜的是现实中这是不可能的。
现在想法是有了，但是怎样去实现它呢？我们知道一台机器是不容易出现问题的，一台电脑是可以陪伴我们很多年的。但是在一个具有成千上万台机器的分布式集群中出现问题确实非常常见的事，所以人们只能尽可能保证容错，而容错机制主要包括下面三个特性：
 可用性（Availability）：我们要能用这个系统，但是它又时常会出问题，那我们只能定期去把数据备份起来了，所以备份可以保证这个系统是可用的； 可恢复性（Recoverability)：我们已经备份了数据，那就要保证在出问题的时候能恢复到出错前的状态。这里可用的一些操作就是更新日志，设置一些检查点，使用非易失性的存储介质（如硬盘），不能一不小心停电了然后什么都没了。 一致性（Consistency)：我们现在有了备份，那多个副本中的数据是不是能保证一致呢？比如在一个&amp;lt;key,value&amp;gt;的数据库中，张三要取1000元钱，然后发送请求导致主数据库更新了数据，但是就在发送更新副本的时候网断了，这就导致了数据的不一致。  而且就备份来说，它应该距离主体非常远，如果两台机器共用一个电源然后断电了或者该地发生了不可预见的地震，后果都是难以承受的。另外分布式系统作为一个基础设施，它有三个很重要的组成，就是存储系统，通信系统和计算系统。
MapReduce MapReduce是谷歌大数据三驾马车中的其中一篇论文，它被提出的目的是用来解决海量网页数据索引和排序之类的问题。它包含Map和Reduce两个过程，以WordCount为例，它的计算过程如下图所示：
当我们面临着一个大文件的时候，谷歌的文件系统GFS(Google File System)会把它差分成多个64kb的块，用来做任务分配的master服务器知道它们具体在哪台机器上，然后它会指定work machine去执行Map的操作。加入这些文件块的存放位置和work machine不是同一台机器，那就需要通过网络通信来进行数据交换，可是这样会导致效率下降，所以一般都会将它们放在同一台机器上，work machine只需要做一个文件读取的操作就可以了。Map(k,v)的v指的就是某一个块的全部文字，所以下面只需要将单词拆分开然后变成key,1的形式就可以了。
图片来源：Google MapReduce论文
在整个Map的过程中master是不需要去一致盯着每台机器去做运算的，每个work machine会将自己运算的中间结果保存在自己的本地磁盘上。以上图中的INPUT 1为例，它最终会保存（a,1)和(b,1)两个值。
然后是Reduce的过程，MapReduce worker通过网络收集所有完成了map worker中&amp;lt;key(i), 1&amp;gt;的任务task，从存储在各个Map worker磁盘上收集数据。这里并不要求每一个Map的所有任务都完成，如果只请求(a,1)这个Map过程，只要work machine完成了这一个任务，就可以将结果返回，不用等全部工作完成。Reduce的伪代码可以表示如下：
图片来源：Google MapReduce论文
它的实现相对简单一点，只是对这些收集到的信息做一个加总的操作，计算出长度就行了。</description>
    </item>
    
  </channel>
</rss>
